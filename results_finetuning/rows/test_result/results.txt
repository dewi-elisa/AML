Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: C:\Users\twank\anaconda3\envs\DewiAML\lib\site-packages\lpips\weights\v0.1\alex.pth
Namespace(batch_size=8, conv_on_input=0, dataset_name='mnist', decouple_beta=0.1, device='cuda', display_interval=100, filter_size=5, gen_frm_dir='results_finetuning/rows', img_channel=1, img_width=64, injection_action='concat', input_length=10, is_training=1, layer_norm=0, lr=0.0001, max_iterations=8000, model_name='predrnn_v2', n_gpu=1, noise='rows', noise_ratio=0.1, noise_size=10, num_action_ch=4, num_hidden='128,128,128,128', num_save_samples=10, parameters=['conv_last.weight'], patch_size=4, pretrained_model='./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-blocks2', r_exp_alpha=2500, r_sampling_step_1=25000.0, r_sampling_step_2=50000, res_on_conv=0, reverse_input=1, reverse_scheduled_sampling=1, sampling_changing_rate=2e-05, sampling_start_value=1.0, sampling_stop_iter=50000, save_dir='checkpoints_finetuning/mnist_predrnn_v2', scheduled_sampling=1, snapshot_interval=10000, stride=1, test_interval=10000, total_length=20, train_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-train.npz', valid_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-test.npz', visual=0, visual_path='./decoupling_visual')
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
clips
(2, 10000, 2)
dims
(1, 3)
input_raw_data
(200000, 1, 64, 64)
load model: ./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-blocks2
Updating 1 parameter(s): ['conv_last.weight']
2025-07-05 19:57:39 itr: 100
training loss: 0.02632286213338375
2025-07-05 19:59:42 itr: 200
training loss: 0.026101307943463326
2025-07-05 20:01:46 itr: 300
training loss: 0.024839002639055252
2025-07-05 20:03:50 itr: 400
training loss: 0.024389784783124924
2025-07-05 20:05:54 itr: 500
training loss: 0.025395222008228302
2025-07-05 20:07:58 itr: 600
training loss: 0.025797218084335327
2025-07-05 20:10:02 itr: 700
training loss: 0.026599964126944542
2025-07-05 20:12:06 itr: 800
training loss: 0.02501954510807991
2025-07-05 20:14:10 itr: 900
training loss: 0.027341006323695183
2025-07-05 20:16:14 itr: 1000
training loss: 0.025254031643271446
2025-07-05 20:18:18 itr: 1100
training loss: 0.02692556381225586
2025-07-05 20:20:21 itr: 1200
training loss: 0.026237107813358307
2025-07-05 20:22:25 itr: 1300
training loss: 0.027580399066209793
2025-07-05 20:24:29 itr: 1400
training loss: 0.02554541826248169
2025-07-05 20:26:33 itr: 1500
training loss: 0.023688793182373047
2025-07-05 20:28:37 itr: 1600
training loss: 0.024878110736608505
2025-07-05 20:30:41 itr: 1700
training loss: 0.026394393295049667
2025-07-05 20:32:45 itr: 1800
training loss: 0.02643745020031929
2025-07-05 20:34:49 itr: 1900
training loss: 0.027066007256507874
2025-07-05 20:36:52 itr: 2000
training loss: 0.025166496634483337
2025-07-05 20:38:56 itr: 2100
training loss: 0.028243761509656906
2025-07-05 20:41:00 itr: 2200
training loss: 0.026218200102448463
2025-07-05 20:43:04 itr: 2300
training loss: 0.026171429082751274
2025-07-05 20:45:08 itr: 2400
training loss: 0.0267813540995121
2025-07-05 20:47:12 itr: 2500
training loss: 0.024684708565473557
2025-07-05 20:49:16 itr: 2600
training loss: 0.029504915699362755
2025-07-05 20:51:19 itr: 2700
training loss: 0.025485537946224213
2025-07-05 20:53:23 itr: 2800
training loss: 0.026516856625676155
2025-07-05 20:55:27 itr: 2900
training loss: 0.025073422119021416
2025-07-05 20:57:31 itr: 3000
training loss: 0.024887893348932266
2025-07-05 20:59:35 itr: 3100
training loss: 0.025743894279003143
2025-07-05 21:01:39 itr: 3200
training loss: 0.025871455669403076
2025-07-05 21:03:43 itr: 3300
training loss: 0.026259073987603188
2025-07-05 21:05:47 itr: 3400
training loss: 0.025862157344818115
2025-07-05 21:07:51 itr: 3500
training loss: 0.02549385465681553
2025-07-05 21:09:55 itr: 3600
training loss: 0.02623596228659153
2025-07-05 21:11:58 itr: 3700
training loss: 0.025518763810396194
2025-07-05 21:14:02 itr: 3800
training loss: 0.026119939982891083
2025-07-05 21:16:06 itr: 3900
training loss: 0.026213165372610092
2025-07-05 21:18:10 itr: 4000
training loss: 0.0259858425706625
2025-07-05 21:20:14 itr: 4100
training loss: 0.025335658341646194
2025-07-05 21:22:18 itr: 4200
training loss: 0.028083618730306625
2025-07-05 21:24:21 itr: 4300
training loss: 0.02653557062149048
2025-07-05 21:26:25 itr: 4400
training loss: 0.024493511766195297
2025-07-05 21:28:29 itr: 4500
training loss: 0.027104733511805534
2025-07-05 21:30:33 itr: 4600
training loss: 0.026034202426671982
2025-07-05 21:32:37 itr: 4700
training loss: 0.02731570601463318
2025-07-05 21:34:41 itr: 4800
training loss: 0.024198750033974648
2025-07-05 21:36:44 itr: 4900
training loss: 0.030194558203220367
2025-07-05 21:38:48 itr: 5000
training loss: 0.023786446079611778
2025-07-05 21:40:52 itr: 5100
training loss: 0.024914320558309555
2025-07-05 21:42:56 itr: 5200
training loss: 0.027820497751235962
2025-07-05 21:45:00 itr: 5300
training loss: 0.027902130037546158
2025-07-05 21:47:04 itr: 5400
training loss: 0.023518994450569153
2025-07-05 21:49:08 itr: 5500
training loss: 0.027302000671625137
2025-07-05 21:51:12 itr: 5600
training loss: 0.024262849241495132
2025-07-05 21:53:16 itr: 5700
training loss: 0.024402311071753502
2025-07-05 21:55:19 itr: 5800
training loss: 0.024499211460351944
2025-07-05 21:57:25 itr: 5900
training loss: 0.025251638144254684
2025-07-05 21:59:29 itr: 6000
training loss: 0.026021257042884827
2025-07-05 22:01:33 itr: 6100
training loss: 0.027057398110628128
2025-07-05 22:03:37 itr: 6200
training loss: 0.026583051308989525
2025-07-05 22:05:41 itr: 6300
training loss: 0.02426445484161377
2025-07-05 22:07:45 itr: 6400
training loss: 0.026266809552907944
2025-07-05 22:09:49 itr: 6500
training loss: 0.02591071091592312
2025-07-05 22:11:53 itr: 6600
training loss: 0.02360237017273903
2025-07-05 22:13:57 itr: 6700
training loss: 0.028933165594935417
2025-07-05 22:16:00 itr: 6800
training loss: 0.025903813540935516
2025-07-05 22:18:04 itr: 6900
training loss: 0.027533922344446182
2025-07-05 22:20:08 itr: 7000
training loss: 0.025294452905654907
2025-07-05 22:22:12 itr: 7100
training loss: 0.026568980887532234
2025-07-05 22:24:16 itr: 7200
training loss: 0.026915190741419792
2025-07-05 22:26:20 itr: 7300
training loss: 0.024097658693790436
2025-07-05 22:28:24 itr: 7400
training loss: 0.024947352707386017
2025-07-05 22:30:28 itr: 7500
training loss: 0.02657328173518181
2025-07-05 22:32:32 itr: 7600
training loss: 0.023897744715213776
2025-07-05 22:34:36 itr: 7700
training loss: 0.026051491498947144
2025-07-05 22:36:40 itr: 7800
training loss: 0.025530897080898285
2025-07-05 22:38:44 itr: 7900
training loss: 0.02652931958436966
2025-07-05 22:40:48 itr: 8000
training loss: 0.024858035147190094
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
2025-07-05 22:40:50 test...
mse per seq: 525.8872297409384
26.37310691568303
32.4481889296343
38.85548785918537
44.73943702167368
51.14533058207303
56.614151143772716
62.171707357314816
66.69484487319374
71.29742161347905
75.54755344492867
ssim per frame: 0.8750079
0.934349
0.92127967
0.908441
0.8964439
0.88301617
0.8698463
0.855057
0.8411972
0.82685107
0.8135975
psnr per frame: 19.553219
22.290527
21.426876
20.661602
20.066206
19.493107
19.042397
18.606745
18.28066
17.963799
17.70029
lpips per frame: 0.084557876
0.04499389
0.0528922
0.059703067
0.066713795
0.07486084
0.08519662
0.09682084
0.10879641
0.12212233
0.1334788
save model to checkpoints_finetuning/mnist_predrnn_v2\model.ckpt-8000-rows