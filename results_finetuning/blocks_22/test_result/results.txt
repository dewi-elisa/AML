Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: C:\Users\twank\anaconda3\envs\DewiAML\lib\site-packages\lpips\weights\v0.1\alex.pth
Namespace(batch_size=8, conv_on_input=0, dataset_name='mnist', decouple_beta=0.1, device='cuda', display_interval=100, filter_size=5, gen_frm_dir='results_finetuning/blocks_22', img_channel=1, img_width=64, injection_action='concat', input_length=10, is_training=1, layer_norm=0, lr=0.0001, max_iterations=8000, model_name='predrnn_v2', n_gpu=1, noise='blocks', noise_ratio=0.1, noise_size=2, num_action_ch=4, num_hidden='128,128,128,128', num_save_samples=10, parameters=['conv_last.weight'], patch_size=4, pretrained_model='./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-random', r_exp_alpha=2500, r_sampling_step_1=25000.0, r_sampling_step_2=50000, res_on_conv=0, reverse_input=1, reverse_scheduled_sampling=1, sampling_changing_rate=2e-05, sampling_start_value=1.0, sampling_stop_iter=50000, save_dir='checkpoints_finetuning/mnist_predrnn_v2', scheduled_sampling=1, snapshot_interval=10000, stride=1, test_interval=10000, total_length=20, train_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-train.npz', valid_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-test.npz', visual=0, visual_path='./decoupling_visual')
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
clips
(2, 10000, 2)
dims
(1, 3)
input_raw_data
(200000, 1, 64, 64)
load model: ./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-random
Updating 1 parameter(s): ['conv_last.weight']
2025-07-05 13:47:43 itr: 100
training loss: 0.026252325624227524
2025-07-05 13:49:49 itr: 200
training loss: 0.02460123598575592
2025-07-05 13:51:55 itr: 300
training loss: 0.023759184405207634
2025-07-05 13:54:04 itr: 400
training loss: 0.02582676149904728
2025-07-05 13:56:11 itr: 500
training loss: 0.027187323197722435
2025-07-05 13:58:17 itr: 600
training loss: 0.025022000074386597
2025-07-05 14:00:25 itr: 700
training loss: 0.025629684329032898
2025-07-05 14:02:31 itr: 800
training loss: 0.026067381724715233
2025-07-05 14:04:37 itr: 900
training loss: 0.029955364763736725
2025-07-05 14:06:44 itr: 1000
training loss: 0.025687387213110924
2025-07-05 14:08:50 itr: 1100
training loss: 0.026019539684057236
2025-07-05 14:10:56 itr: 1200
training loss: 0.02702656015753746
2025-07-05 14:13:02 itr: 1300
training loss: 0.025026638060808182
2025-07-05 14:15:09 itr: 1400
training loss: 0.024908315390348434
2025-07-05 14:17:15 itr: 1500
training loss: 0.028018325567245483
2025-07-05 14:19:21 itr: 1600
training loss: 0.02483251318335533
2025-07-05 14:21:27 itr: 1700
training loss: 0.025700444355607033
2025-07-05 14:23:33 itr: 1800
training loss: 0.02592739835381508
2025-07-05 14:25:39 itr: 1900
training loss: 0.025444846600294113
2025-07-05 14:27:45 itr: 2000
training loss: 0.027578670531511307
2025-07-05 14:29:51 itr: 2100
training loss: 0.025184178724884987
2025-07-05 14:31:57 itr: 2200
training loss: 0.02531149983406067
2025-07-05 14:34:03 itr: 2300
training loss: 0.027964230626821518
2025-07-05 14:36:09 itr: 2400
training loss: 0.025936763733625412
2025-07-05 14:38:16 itr: 2500
training loss: 0.025669502094388008
2025-07-05 14:40:22 itr: 2600
training loss: 0.025940071791410446
2025-07-05 14:42:28 itr: 2700
training loss: 0.026234963908791542
2025-07-05 14:44:34 itr: 2800
training loss: 0.024592505767941475
2025-07-05 14:46:41 itr: 2900
training loss: 0.024215830489993095
2025-07-05 14:48:47 itr: 3000
training loss: 0.02527477592229843
2025-07-05 14:50:53 itr: 3100
training loss: 0.02795376069843769
2025-07-05 14:52:59 itr: 3200
training loss: 0.02396239899098873
2025-07-05 14:55:05 itr: 3300
training loss: 0.027687862515449524
2025-07-05 14:57:11 itr: 3400
training loss: 0.02653493545949459
2025-07-05 14:59:17 itr: 3500
training loss: 0.025039304047822952
2025-07-05 15:01:23 itr: 3600
training loss: 0.02534571662545204
2025-07-05 15:03:29 itr: 3700
training loss: 0.02458111383020878
2025-07-05 15:05:35 itr: 3800
training loss: 0.023430796340107918
2025-07-05 15:07:41 itr: 3900
training loss: 0.025291040539741516
2025-07-05 15:09:47 itr: 4000
training loss: 0.025097375735640526
2025-07-05 15:11:53 itr: 4100
training loss: 0.0261293426156044
2025-07-05 15:13:59 itr: 4200
training loss: 0.023915814235806465
2025-07-05 15:16:05 itr: 4300
training loss: 0.02470199018716812
2025-07-05 15:18:11 itr: 4400
training loss: 0.025478236377239227
2025-07-05 15:20:17 itr: 4500
training loss: 0.02500295825302601
2025-07-05 15:22:23 itr: 4600
training loss: 0.02409772202372551
2025-07-05 15:24:30 itr: 4700
training loss: 0.02620280534029007
2025-07-05 15:26:35 itr: 4800
training loss: 0.025214754045009613
2025-07-05 15:28:42 itr: 4900
training loss: 0.026469029486179352
2025-07-05 15:30:48 itr: 5000
training loss: 0.02926476113498211
2025-07-05 15:32:54 itr: 5100
training loss: 0.02448323369026184
2025-07-05 15:35:00 itr: 5200
training loss: 0.02530619315803051
2025-07-05 15:37:06 itr: 5300
training loss: 0.025187425315380096
2025-07-05 15:39:12 itr: 5400
training loss: 0.025844838470220566
2025-07-05 15:41:18 itr: 5500
training loss: 0.025224095210433006
2025-07-05 15:43:24 itr: 5600
training loss: 0.023809537291526794
2025-07-05 15:45:31 itr: 5700
training loss: 0.028261056169867516
2025-07-05 15:47:36 itr: 5800
training loss: 0.025621185079216957
2025-07-05 15:49:42 itr: 5900
training loss: 0.027208665385842323
2025-07-05 15:51:48 itr: 6000
training loss: 0.02457367070019245
2025-07-05 15:53:55 itr: 6100
training loss: 0.02657836675643921
2025-07-05 15:56:01 itr: 6200
training loss: 0.028066666796803474
2025-07-05 15:58:07 itr: 6300
training loss: 0.026517584919929504
2025-07-05 16:00:13 itr: 6400
training loss: 0.026317639276385307
2025-07-05 16:02:19 itr: 6500
training loss: 0.02698460780084133
2025-07-05 16:04:26 itr: 6600
training loss: 0.02459925226867199
2025-07-05 16:06:32 itr: 6700
training loss: 0.028362153097987175
2025-07-05 16:08:38 itr: 6800
training loss: 0.025421911850571632
2025-07-05 16:10:45 itr: 6900
training loss: 0.026749536395072937
2025-07-05 16:12:51 itr: 7000
training loss: 0.026490047574043274
2025-07-05 16:14:57 itr: 7100
training loss: 0.024644028395414352
2025-07-05 16:17:03 itr: 7200
training loss: 0.024966605007648468
2025-07-05 16:19:09 itr: 7300
training loss: 0.026824867352843285
2025-07-05 16:21:15 itr: 7400
training loss: 0.02544175460934639
2025-07-05 16:23:21 itr: 7500
training loss: 0.025417592376470566
2025-07-05 16:25:27 itr: 7600
training loss: 0.025249943137168884
2025-07-05 16:27:33 itr: 7700
training loss: 0.02570188418030739
2025-07-05 16:29:39 itr: 7800
training loss: 0.0242452509701252
2025-07-05 16:31:46 itr: 7900
training loss: 0.024022217839956284
2025-07-05 16:33:52 itr: 8000
training loss: 0.024604281410574913
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
2025-07-05 16:33:54 test...
mse per seq: 518.118703026185
26.54672679799126
32.14088505219648
38.375536551450026
44.02707535442821
50.19944843251437
55.575704564385234
60.83051318408334
65.59182175213003
70.31691605267041
74.5140752843357
ssim per frame: 0.87640315
0.933856
0.9217821
0.90917194
0.89765716
0.8846688
0.8717428
0.857568
0.843267
0.82879066
0.8155272
psnr per frame: 19.588686
22.208775
21.433628
20.69813
20.103104
19.551981
19.102802
18.686338
18.332733
18.015165
17.754202
lpips per frame: 0.083635055
0.045167036
0.05263852
0.059266392
0.06610583
0.073911496
0.083580464
0.09539845
0.10769018
0.12065715
0.13193509
save model to checkpoints_finetuning/mnist_predrnn_v2\model.ckpt-8000-blocks2