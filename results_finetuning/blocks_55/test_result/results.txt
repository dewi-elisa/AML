Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: C:\Users\twank\anaconda3\envs\DewiAML\lib\site-packages\lpips\weights\v0.1\alex.pth
Namespace(batch_size=8, conv_on_input=0, dataset_name='mnist', decouple_beta=0.1, device='cuda', display_interval=100, filter_size=5, gen_frm_dir='results_finetuning/blocks_55', img_channel=1, img_width=64, injection_action='concat', input_length=10, is_training=1, layer_norm=0, lr=0.0001, max_iterations=8000, model_name='predrnn_v2', n_gpu=1, noise='blocks', noise_ratio=0.1, noise_size=5, num_action_ch=4, num_hidden='128,128,128,128', num_save_samples=10, parameters=['conv_last.weight'], patch_size=4, pretrained_model='./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-rows', r_exp_alpha=2500, r_sampling_step_1=25000.0, r_sampling_step_2=50000, res_on_conv=0, reverse_input=1, reverse_scheduled_sampling=1, sampling_changing_rate=2e-05, sampling_start_value=1.0, sampling_stop_iter=50000, save_dir='checkpoints_finetuning/mnist_predrnn_v2', scheduled_sampling=1, snapshot_interval=10000, stride=1, test_interval=10000, total_length=20, train_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-train.npz', valid_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-test.npz', visual=0, visual_path='./decoupling_visual')
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
clips
(2, 10000, 2)
dims
(1, 3)
input_raw_data
(200000, 1, 64, 64)
load model: ./checkpoints_finetuning/mnist_predrnn_v2/model.ckpt-8000-rows
Updating 1 parameter(s): ['conv_last.weight']
2025-07-05 22:46:21 itr: 100
training loss: 0.02850726619362831
2025-07-05 22:48:25 itr: 200
training loss: 0.027269970625638962
2025-07-05 22:50:29 itr: 300
training loss: 0.02754795551300049
2025-07-05 22:52:33 itr: 400
training loss: 0.028041677549481392
2025-07-05 22:54:37 itr: 500
training loss: 0.026271698996424675
2025-07-05 22:56:41 itr: 600
training loss: 0.025878462940454483
2025-07-05 22:58:45 itr: 700
training loss: 0.027100875973701477
2025-07-05 23:00:50 itr: 800
training loss: 0.02635527029633522
2025-07-05 23:02:54 itr: 900
training loss: 0.026976177468895912
2025-07-05 23:04:58 itr: 1000
training loss: 0.02520996704697609
2025-07-05 23:07:02 itr: 1100
training loss: 0.029483579099178314
2025-07-05 23:09:06 itr: 1200
training loss: 0.027002209797501564
2025-07-05 23:11:10 itr: 1300
training loss: 0.02880149707198143
2025-07-05 23:13:14 itr: 1400
training loss: 0.02627406269311905
2025-07-05 23:15:18 itr: 1500
training loss: 0.0253605954349041
2025-07-05 23:17:22 itr: 1600
training loss: 0.02692439593374729
2025-07-05 23:19:26 itr: 1700
training loss: 0.026552805677056313
2025-07-05 23:21:30 itr: 1800
training loss: 0.02629248984158039
2025-07-05 23:23:34 itr: 1900
training loss: 0.026381978765130043
2025-07-05 23:25:38 itr: 2000
training loss: 0.025066334754228592
2025-07-05 23:27:42 itr: 2100
training loss: 0.03011995181441307
2025-07-05 23:29:46 itr: 2200
training loss: 0.025724802166223526
2025-07-05 23:31:50 itr: 2300
training loss: 0.02690713107585907
2025-07-05 23:33:54 itr: 2400
training loss: 0.027076520025730133
2025-07-05 23:35:58 itr: 2500
training loss: 0.02606612630188465
2025-07-05 23:38:02 itr: 2600
training loss: 0.02624760940670967
2025-07-05 23:40:06 itr: 2700
training loss: 0.026483554393053055
2025-07-05 23:42:10 itr: 2800
training loss: 0.027949508279561996
2025-07-05 23:44:14 itr: 2900
training loss: 0.02819925919175148
2025-07-05 23:46:18 itr: 3000
training loss: 0.029129130765795708
2025-07-05 23:48:23 itr: 3100
training loss: 0.024773433804512024
2025-07-05 23:50:27 itr: 3200
training loss: 0.026326604187488556
2025-07-05 23:52:31 itr: 3300
training loss: 0.0279218889772892
2025-07-05 23:54:35 itr: 3400
training loss: 0.027641361579298973
2025-07-05 23:56:39 itr: 3500
training loss: 0.02629285678267479
2025-07-05 23:58:43 itr: 3600
training loss: 0.0289675984531641
2025-07-06 00:00:47 itr: 3700
training loss: 0.028238877654075623
2025-07-06 00:02:51 itr: 3800
training loss: 0.02809206023812294
2025-07-06 00:04:55 itr: 3900
training loss: 0.026888560503721237
2025-07-06 00:06:59 itr: 4000
training loss: 0.02693096175789833
2025-07-06 00:09:03 itr: 4100
training loss: 0.0255744569003582
2025-07-06 00:11:07 itr: 4200
training loss: 0.027290306985378265
2025-07-06 00:13:11 itr: 4300
training loss: 0.028375964611768723
2025-07-06 00:15:15 itr: 4400
training loss: 0.027107786387205124
2025-07-06 00:17:19 itr: 4500
training loss: 0.02762192115187645
2025-07-06 00:19:23 itr: 4600
training loss: 0.026835285127162933
2025-07-06 00:21:27 itr: 4700
training loss: 0.027141433209180832
2025-07-06 00:23:31 itr: 4800
training loss: 0.027279198169708252
2025-07-06 00:25:35 itr: 4900
training loss: 0.026374511420726776
2025-07-06 00:27:40 itr: 5000
training loss: 0.027371736243367195
2025-07-06 00:29:46 itr: 5100
training loss: 0.027045823633670807
2025-07-06 00:31:50 itr: 5200
training loss: 0.026452306658029556
2025-07-06 00:33:54 itr: 5300
training loss: 0.02772993966937065
2025-07-06 00:35:59 itr: 5400
training loss: 0.029104871675372124
2025-07-06 00:38:03 itr: 5500
training loss: 0.02555326372385025
2025-07-06 00:40:07 itr: 5600
training loss: 0.029001586139202118
2025-07-06 00:42:11 itr: 5700
training loss: 0.02649076282978058
2025-07-06 00:44:15 itr: 5800
training loss: 0.026321228593587875
2025-07-06 00:46:19 itr: 5900
training loss: 0.025304868817329407
2025-07-06 00:48:24 itr: 6000
training loss: 0.02671578899025917
2025-07-06 00:50:28 itr: 6100
training loss: 0.025808576494455338
2025-07-06 00:52:32 itr: 6200
training loss: 0.026300184428691864
2025-07-06 00:54:36 itr: 6300
training loss: 0.026677150279283524
2025-07-06 00:56:40 itr: 6400
training loss: 0.02587643638253212
2025-07-06 00:58:44 itr: 6500
training loss: 0.026289859786629677
2025-07-06 01:00:48 itr: 6600
training loss: 0.026346661150455475
2025-07-06 01:02:52 itr: 6700
training loss: 0.02614331617951393
2025-07-06 01:04:56 itr: 6800
training loss: 0.02537950500845909
2025-07-06 01:07:01 itr: 6900
training loss: 0.026346663013100624
2025-07-06 01:09:05 itr: 7000
training loss: 0.025123687461018562
2025-07-06 01:11:09 itr: 7100
training loss: 0.025604547932744026
2025-07-06 01:13:13 itr: 7200
training loss: 0.027588795870542526
2025-07-06 01:15:17 itr: 7300
training loss: 0.026580557227134705
2025-07-06 01:17:21 itr: 7400
training loss: 0.027210894972085953
2025-07-06 01:19:25 itr: 7500
training loss: 0.025682318955659866
2025-07-06 01:21:29 itr: 7600
training loss: 0.027489956468343735
2025-07-06 01:23:33 itr: 7700
training loss: 0.02515910193324089
2025-07-06 01:25:37 itr: 7800
training loss: 0.03026575595140457
2025-07-06 01:27:42 itr: 7900
training loss: 0.025045104324817657
2025-07-06 01:29:46 itr: 8000
training loss: 0.026369178667664528
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
2025-07-06 01:29:48 test...
mse per seq: 649.3913909024733
34.70128501545299
42.630442573424965
50.70103895217977
57.988617269750584
64.36064312164797
70.36996994426543
75.8867482476056
80.34578184647994
84.54851486083658
87.8583490708295
ssim per frame: 0.8493328
0.9185415
0.9020647
0.8859965
0.871215
0.85694957
0.8418468
0.8259472
0.8109128
0.7963401
0.7835142
psnr per frame: 18.53862
21.099138
20.235594
19.481894
18.883282
18.433699
18.032894
17.680973
17.404646
17.161028
16.973072
lpips per frame: 0.104819655
0.058914866
0.06838289
0.07622471
0.084098384
0.09430675
0.10524017
0.118581615
0.13295172
0.14807999
0.16141541
save model to checkpoints_finetuning/mnist_predrnn_v2\model.ckpt-8000-blocks5