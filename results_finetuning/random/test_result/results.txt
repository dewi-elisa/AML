Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: C:\Users\twank\anaconda3\envs\DewiAML\lib\site-packages\lpips\weights\v0.1\alex.pth
Namespace(batch_size=8, conv_on_input=0, dataset_name='mnist', decouple_beta=0.1, device='cuda', display_interval=100, filter_size=5, gen_frm_dir='results_finetuning/random', img_channel=1, img_width=64, injection_action='concat', input_length=10, is_training=1, layer_norm=0, lr=0.0001, max_iterations=8000, model_name='predrnn_v2', n_gpu=1, noise='random', noise_ratio=0.1, noise_size=10, num_action_ch=4, num_hidden='128,128,128,128', num_save_samples=10, parameters=['conv_last.weight'], patch_size=4, pretrained_model='./checkpointss/mnist_predrnn_v2/mnist_model.ckpt', r_exp_alpha=2500, r_sampling_step_1=25000.0, r_sampling_step_2=50000, res_on_conv=0, reverse_input=1, reverse_scheduled_sampling=1, sampling_changing_rate=2e-05, sampling_start_value=1.0, sampling_stop_iter=50000, save_dir='checkpoints_finetuning/mnist_predrnn_v2', scheduled_sampling=1, snapshot_interval=10000, stride=1, test_interval=10000, total_length=20, train_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-train.npz', valid_data_paths='C:/Users/twank/Documents/Dewi/predrnn-pytorch-master/predrnn-pytorch-master/moving-mnist-example/moving-mnist-test.npz', visual=0, visual_path='./decoupling_visual')
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
clips
(2, 10000, 2)
dims
(1, 3)
input_raw_data
(200000, 1, 64, 64)
load model: ./checkpointss/mnist_predrnn_v2/mnist_model.ckpt
Updating 1 parameter(s): ['conv_last.weight']
2025-07-05 10:55:41 itr: 100
training loss: 0.026442397385835648
2025-07-05 10:57:49 itr: 200
training loss: 0.025539692491292953
2025-07-05 10:59:56 itr: 300
training loss: 0.02444806694984436
2025-07-05 11:02:04 itr: 400
training loss: 0.0241845790296793
2025-07-05 11:04:11 itr: 500
training loss: 0.024884473532438278
2025-07-05 11:06:18 itr: 600
training loss: 0.024602513760328293
2025-07-05 11:08:25 itr: 700
training loss: 0.026313941925764084
2025-07-05 11:10:34 itr: 800
training loss: 0.024677667766809464
2025-07-05 11:12:39 itr: 900
training loss: 0.023459505289793015
2025-07-05 11:14:45 itr: 1000
training loss: 0.025853170081973076
2025-07-05 11:16:50 itr: 1100
training loss: 0.026957347989082336
2025-07-05 11:18:56 itr: 1200
training loss: 0.024653609842061996
2025-07-05 11:21:01 itr: 1300
training loss: 0.025104619562625885
2025-07-05 11:23:06 itr: 1400
training loss: 0.024993330240249634
2025-07-05 11:25:11 itr: 1500
training loss: 0.024587130174040794
2025-07-05 11:27:17 itr: 1600
training loss: 0.02643636427819729
2025-07-05 11:29:22 itr: 1700
training loss: 0.024482181295752525
2025-07-05 11:31:27 itr: 1800
training loss: 0.025532696396112442
2025-07-05 11:33:32 itr: 1900
training loss: 0.0260225310921669
2025-07-05 11:35:37 itr: 2000
training loss: 0.026757866144180298
2025-07-05 11:37:42 itr: 2100
training loss: 0.025371963158249855
2025-07-05 11:39:47 itr: 2200
training loss: 0.026713017374277115
2025-07-05 11:41:52 itr: 2300
training loss: 0.024893639609217644
2025-07-05 11:43:57 itr: 2400
training loss: 0.023755257949233055
2025-07-05 11:46:03 itr: 2500
training loss: 0.02515999600291252
2025-07-05 11:48:08 itr: 2600
training loss: 0.02785949781537056
2025-07-05 11:50:13 itr: 2700
training loss: 0.023096023127436638
2025-07-05 11:52:18 itr: 2800
training loss: 0.025269443169236183
2025-07-05 11:54:22 itr: 2900
training loss: 0.024423401802778244
2025-07-05 11:56:27 itr: 3000
training loss: 0.02622467465698719
2025-07-05 11:58:33 itr: 3100
training loss: 0.024917760863900185
2025-07-05 12:00:42 itr: 3200
training loss: 0.02444508671760559
2025-07-05 12:02:51 itr: 3300
training loss: 0.024713506922125816
2025-07-05 12:04:59 itr: 3400
training loss: 0.027506813406944275
2025-07-05 12:07:07 itr: 3500
training loss: 0.02678225003182888
2025-07-05 12:09:13 itr: 3600
training loss: 0.02454080805182457
2025-07-05 12:11:18 itr: 3700
training loss: 0.02431611716747284
2025-07-05 12:13:23 itr: 3800
training loss: 0.02395319938659668
2025-07-05 12:15:28 itr: 3900
training loss: 0.02566326968371868
2025-07-05 12:17:33 itr: 4000
training loss: 0.02473871037364006
2025-07-05 12:19:39 itr: 4100
training loss: 0.023537050932645798
2025-07-05 12:21:47 itr: 4200
training loss: 0.025012899190187454
2025-07-05 12:23:56 itr: 4300
training loss: 0.027274902909994125
2025-07-05 12:26:05 itr: 4400
training loss: 0.023939508944749832
2025-07-05 12:28:13 itr: 4500
training loss: 0.02607608586549759
2025-07-05 12:30:21 itr: 4600
training loss: 0.02446335181593895
2025-07-05 12:32:30 itr: 4700
training loss: 0.026562374085187912
2025-07-05 12:34:40 itr: 4800
training loss: 0.024937426671385765
2025-07-05 12:36:50 itr: 4900
training loss: 0.025300992652773857
2025-07-05 12:38:59 itr: 5000
training loss: 0.02482917718589306
2025-07-05 12:41:07 itr: 5100
training loss: 0.02659573405981064
2025-07-05 12:43:13 itr: 5200
training loss: 0.02350691705942154
2025-07-05 12:45:18 itr: 5300
training loss: 0.027671881020069122
2025-07-05 12:47:24 itr: 5400
training loss: 0.02648385986685753
2025-07-05 12:49:33 itr: 5500
training loss: 0.024239741265773773
2025-07-05 12:51:42 itr: 5600
training loss: 0.02540963888168335
2025-07-05 12:53:52 itr: 5700
training loss: 0.024240724742412567
2025-07-05 12:55:59 itr: 5800
training loss: 0.025291822850704193
2025-07-05 12:58:05 itr: 5900
training loss: 0.02533646672964096
2025-07-05 13:00:10 itr: 6000
training loss: 0.02507779933512211
2025-07-05 13:02:15 itr: 6100
training loss: 0.02604695037007332
2025-07-05 13:04:20 itr: 6200
training loss: 0.024657435715198517
2025-07-05 13:06:25 itr: 6300
training loss: 0.027109414339065552
2025-07-05 13:08:30 itr: 6400
training loss: 0.024674275889992714
2025-07-05 13:10:36 itr: 6500
training loss: 0.028285477310419083
2025-07-05 13:12:41 itr: 6600
training loss: 0.02597116492688656
2025-07-05 13:14:46 itr: 6700
training loss: 0.0258698221296072
2025-07-05 13:16:51 itr: 6800
training loss: 0.0260377898812294
2025-07-05 13:18:56 itr: 6900
training loss: 0.026116201654076576
2025-07-05 13:21:01 itr: 7000
training loss: 0.02541755512356758
2025-07-05 13:23:06 itr: 7100
training loss: 0.024773554876446724
2025-07-05 13:25:12 itr: 7200
training loss: 0.024623295292258263
2025-07-05 13:27:18 itr: 7300
training loss: 0.026672303676605225
2025-07-05 13:29:23 itr: 7400
training loss: 0.025861511006951332
2025-07-05 13:31:29 itr: 7500
training loss: 0.026773056015372276
2025-07-05 13:33:33 itr: 7600
training loss: 0.026230379939079285
2025-07-05 13:35:38 itr: 7700
training loss: 0.024933714419603348
2025-07-05 13:37:43 itr: 7800
training loss: 0.024765681475400925
2025-07-05 13:39:48 itr: 7900
training loss: 0.024612542241811752
2025-07-05 13:41:54 itr: 8000
training loss: 0.02485577203333378
clips
(2, 3000, 2)
dims
(1, 3)
input_raw_data
(60000, 1, 64, 64)
2025-07-05 13:41:56 test...
mse per seq: 488.8105729352982
24.06105302106888
29.64309321112811
35.487231958358684
41.101712124870424
47.16264769610237
52.30256971573447
57.79622712364809
62.36861859285895
67.31208019970573
71.57533929182246
ssim per frame: 0.88394725
0.93903834
0.92722154
0.91544616
0.90427315
0.89169025
0.87967294
0.8657868
0.8524082
0.8383205
0.8256152
psnr per frame: 19.890802
22.666237
21.816574
21.054064
20.425568
19.843792
19.392622
18.938852
18.583916
18.230318
17.956087
lpips per frame: 0.07728112
0.040566936
0.047742374
0.05430134
0.06080706
0.068678424
0.07780993
0.088282354
0.09993703
0.11203365
0.122652076
save model to checkpoints_finetuning/mnist_predrnn_v2\model.ckpt-8000-random